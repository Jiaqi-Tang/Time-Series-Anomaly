{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60ddcaba",
   "metadata": {},
   "source": [
    "# 1. Dataset Introduction & Loading\n",
    "\n",
    "The dataset `short_seasonal.csv` is a synthetic dataset, data generation code can be found in `src/synthetic_data_generation.ipynb`. It is a small dataset of 80 data points and simulates monthly data with strong seasonality and a positive linear trend. \n",
    "\n",
    "The purpose of this dataset is to provide a small, computationally efficient dataset to test upon. It's short length and strong seasonality also brings unique challenges to data-intensive models with less seasonality adaptions.\n",
    "\n",
    "### Loading data from `csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda9e814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/short_seasonal.csv')\n",
    "\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "ts = df.set_index(\"timestamp\")[\"value\"]\n",
    "ts = ts.asfreq('ME')\n",
    "lables = df.set_index(\"timestamp\")[\"labels\"]\n",
    "lables = lables.asfreq('ME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee0830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization.plots import plot_ts_with_anom\n",
    "\n",
    "plot_ts_with_anom(ts, lables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70184a11",
   "metadata": {},
   "source": [
    "This time series is generated additive model, using a linear trend, period 12 seasonality, and $MA(1)$ noise.\n",
    "\n",
    "The anomalies for this dataset occur from Feb 2019 to Dec 2019. The anomaly values are generated using independent sampling from a normal distribution with `mean=ts.mean()` and `std=2.5`.\n",
    "\n",
    "For the upcoming modeling sections, I will be working only with the original time series **without** the labels. The goal of this report is to investigate how well models detect anomalies on **unlabeled** data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54866e9",
   "metadata": {},
   "source": [
    "# 2. Modeling - Classical Statistics\n",
    "\n",
    "To begin, lets plot the time series to gain an overall understanding of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee758cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization.plots import plot_ts\n",
    "\n",
    "plot_ts(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a2751c",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Strong yearly seasonality, values peek during the first 1-2 months every year\n",
    "- Increasing trend, seems roughly linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98f5c4f",
   "metadata": {},
   "source": [
    "## 2.1 STL Model\n",
    "\n",
    "To begin, lets start with a quick Seasonal-Trend decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2a797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "stl = STL(ts, period=12) \n",
    "stl_result = stl.fit()\n",
    "\n",
    "_ = stl_result.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9b732e",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Mostly linear trend with a light wave shape from 2017-2020\n",
    "- Larger residuals from late 2018 to late 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae5fa03",
   "metadata": {},
   "source": [
    "### Investigation: Linear Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5347b0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.statistical.linear_models import linear_model\n",
    "from src.visualization.plots import plot_fit\n",
    "\n",
    "model, X = linear_model(stl_result.trend)\n",
    "lin_results = model.fit()\n",
    "\n",
    "lin_pred = pd.Series(lin_results.predict(X), index=ts.index)\n",
    "plot_fit(stl_result.trend, lin_pred, title='Linear model fit to Trend')\n",
    "\n",
    "print(lin_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d619c2b",
   "metadata": {},
   "source": [
    "The summary provides evidence that trend should be modeled linearly. With a $p$-value of $0.000$ for all parameters, there is strong evidence against any model parameters being zero, hence no need to drop parameters. The R-squared and Adj. R-squared are both high at $0.968$, showing $96.8\\%$ of the response can be explained by the explanatory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3c15bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization.plots import plot_resid\n",
    "\n",
    "lin_resid = pd.Series(lin_results.resid, index=ts.index)\n",
    "plot_resid(lin_resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa8e54",
   "metadata": {},
   "source": [
    "There is strong trend in the residual plot suggesting that the residuals are not indenpent and identiacally distributed normal. The original time series deviates futhest from the preditions from late 2018 to late 2019 $(|r_i|>0.5)$, suggesting that this may be where the anomalies occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85270e35",
   "metadata": {},
   "source": [
    "### Investigation: Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4861e900",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_STL_resid = (abs(stl_result.resid) > 2)\n",
    "plot_resid(stl_result.resid, hlines=[-2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2807140f",
   "metadata": {},
   "source": [
    "The residuals look independent and evenly spread across zero, with outliers occuring between mid 2018 and late 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21dcdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.statistical.rule_based_functions import cusum\n",
    "\n",
    "# Add plot cusum sum option\n",
    "cusum_resid = cusum(stl_result.resid, stl_result.resid.mean(), threshold=(3 * stl_result.resid.std()), drift=(0.1 * stl_result.resid.std()))\n",
    "plot_ts_with_anom(ts, cusum_resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fe5b93",
   "metadata": {},
   "source": [
    "No significant anomaly clusters are found by running CUSUM on the residuals, showing little change in mean of the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13792dca",
   "metadata": {},
   "source": [
    "## 2.2 SARIMA Model\n",
    "### Differenced Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89b3baf",
   "metadata": {},
   "source": [
    "Base on part 2.1, it is safe to assume that there is a seasonal component of period 12 and a linear trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3374f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_diff = ts.diff(12).diff()\n",
    "plot_ts(ts_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4346cd",
   "metadata": {},
   "source": [
    "Differencing the time series first by the period (to remove seasonalitiy) then by 1-step back in time give the above graph. This differenced times series looks to have constant mean around zero, but with variablilty in variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a402ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sliding window variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d1d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "stationary_pval = adfuller(ts_diff.dropna())[1]\n",
    "print(f'p-value: {stationary_pval}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b8353d",
   "metadata": {},
   "source": [
    "Applying the augmented Dickeyâ€“Fuller test on the differenced time series returns a statistically significant $p$-value, thus there is strong evidence that the time series is stationary (constant mean and variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abae5ed",
   "metadata": {},
   "source": [
    "### SARIMA Model selection\n",
    "\n",
    "Lets begin by plotting the acf/pacf values to get an idea of the auto-correlation for this times series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef504f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from scipy.stats import norm\n",
    "from src.visualization.plots import plot_lag_with_ci\n",
    "import numpy as np\n",
    "\n",
    "nlags = 24 # Covers 2 periods\n",
    "pacf_vals = pacf(ts, nlags=nlags)\n",
    "acf_vals = acf(ts, nlags=nlags, fft=True)\n",
    "\n",
    "conf_interval = norm.ppf(1 - 0.05 / 2) / np.sqrt(len(ts))\n",
    "plot_lag_with_ci(pacf_vals, conf_interval, title='PACF')\n",
    "plot_lag_with_ci(acf_vals, conf_interval, title='ACF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe15e7bf",
   "metadata": {},
   "source": [
    "Both the ACF and PACF plots have a notably high lag at $12$, suggesting $(P, D, Q)=(1, 1, 1)$. \n",
    "\n",
    "The ACF and PACF both seem to be deminished in value over higher lags, but have many statistically significant lags in the first period $(< 12)$. This may be due to small data sample size (total length of $80$ and post-differencing lenght of $67$). For simplicity, only $p, q < 3$ are considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b67d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.statistical.stationary_models import SARIMA_grid_search\n",
    "\n",
    "results_table = SARIMA_grid_search(ts, period=12, d=1, D=1, max_p=3, max_q=3, max_P=1, max_Q=1)\n",
    "df_results = pd.DataFrame(results_table).sort_values('AIC', na_position='last')\n",
    "\n",
    "display(df_results[['order', 'seasonal_order', 'AIC', 'BIC']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba4ccf2",
   "metadata": {},
   "source": [
    "The top 10 models show no significant difference in their `aic` or `bic` scores. Taking both metrics and model complexcity into consideration, I have choosen $SARIMA(0, 1, 1)(0, 1, 1, 12)$ to be the model for further analysis.\n",
    "\n",
    "The choosen model is plotted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c12ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = df_results.iloc[3]\n",
    "plot_fit(ts, fitted_vals=best_model['model'].fittedvalues[13:], CI=best_model['conf_int'], anom=best_model['anom'], title=f'SARMIA{best_model['order']}{best_model['seasonal_order']}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58992e3",
   "metadata": {},
   "source": [
    "### SARIMA residual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f9584",
   "metadata": {},
   "outputs": [],
   "source": [
    "SARIMA_resid = best_model['model'].resid[13:]\n",
    "plot_resid(SARIMA_resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf6483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do rolling window variance + outlier cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b31e2d4",
   "metadata": {},
   "source": [
    "### Across model Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257d144f",
   "metadata": {},
   "source": [
    "Going back to all the SARIMA models fitted during the grid search, we can look at the data points where all models had a hard time predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c4737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.statistical.stationary_models import count_anoms\n",
    "\n",
    "anom_counts = count_anoms(df_results[['anom']].head(10))\n",
    "plot_ts(anom_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bddcabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_counts = count_anoms(df_results[['anom']])\n",
    "plot_ts(anom_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b13589",
   "metadata": {},
   "source": [
    "This shows that models have a significatly harder time predicting data points from early 2019 to early 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efd34f7",
   "metadata": {},
   "source": [
    "## 2.3 BOCPD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28ac267",
   "metadata": {},
   "source": [
    "## 2.4 Kalman Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d81409",
   "metadata": {},
   "source": [
    "# 3 Self Trained Machine Learning models\n",
    "\n",
    "Add Isolation forest and TCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8406a923",
   "metadata": {},
   "source": [
    "## 3.1 LSTM Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68849f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.models.preprocessing import create_sliding_windows, np_to_dataloader\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(13)\n",
    "\n",
    "# Add preprocessing\n",
    "windows = create_sliding_windows(ts, window_size=12)\n",
    "data_loader = np_to_dataloader(windows, batch_size=16, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf84761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.self_trained_ml.lstm_ae import LSTMAutoencoder\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "model = LSTMAutoencoder(hidden_dim=8, latent_dim=8)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513911cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.self_trained_ml.lstm_ae import train_LSTMAE, eval_LTSMAE_MSE\n",
    "import torch\n",
    "\n",
    "train_LSTMAE(model, data_loader, criterion=criterion, optimizer=optimizer, epochs=50)\n",
    "mse_LSTM = eval_LTSMAE_MSE(model, test_data=torch.tensor(windows, dtype=torch.float32).unsqueeze(-1))\n",
    "\n",
    "errors_ts = pd.Series(mse_LSTM.numpy(), index=ts.index[:len(ts)-12])\n",
    "plot_ts(errors_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e3f886",
   "metadata": {},
   "source": [
    "# 4 Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e08e752",
   "metadata": {},
   "source": [
    "## 4.1 Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fcfb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "prophet_df = pd.DataFrame({\n",
    "    'ds': ts.index,  \n",
    "    'y': ts.values \n",
    "})\n",
    "\n",
    "model = Prophet()\n",
    "model.fit(prophet_df)\n",
    "\n",
    "prophet_pred = model.predict(prophet_df[['ds']].copy())\n",
    "\n",
    "prophet_anom = (prophet_df['y'] < prophet_pred['yhat_lower']) | (prophet_df['y'] > prophet_pred['yhat_upper'])\n",
    "prophet_anom = pd.Series(prophet_anom.values, index=ts.index)\n",
    "\n",
    "prophet_CI = prophet_pred[['yhat_lower', 'yhat_upper']].copy()\n",
    "prophet_CI.index = ts.index\n",
    "\n",
    "plot_fit(ts, pd.Series(prophet_pred['yhat'].values, index=ts.index), CI=prophet_CI, anom=prophet_anom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf7c86e",
   "metadata": {},
   "source": [
    "## 4.2 AWS Lookout for Metrics\n",
    "\n",
    "Due to the [data size and constraint requirments](https://docs.aws.amazon.com/lookoutmetrics/latest/dev/detectors-setup.html#:~:text=Detector%20Statuses,data%20requirements.) for the AWS Lookout for Metrics model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdde5c9",
   "metadata": {},
   "source": [
    "AWS_df = df[['timestamp', 'value']].rename(columns={\"value\": \"metric_value\"})\n",
    "print(AWS_df)\n",
    "\n",
    "AWS_csv = './data/AWS_LfM_csv/short_seasonal_AWS.csv'\n",
    "AWS_df.to_csv(AWS_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce8dd72",
   "metadata": {},
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3', region_name='us-east-1')\n",
    "bucket_name = \"time-series-anomaly-project\" \n",
    "\n",
    "s3.upload_file(AWS_csv, bucket_name, 'short_seasonal_AWS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdc059e",
   "metadata": {},
   "source": [
    "client = boto3.client('lookoutmetrics', region_name='us-east-1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee83855",
   "metadata": {},
   "source": [
    "detector_arn = response['AnomalyDetectorArn']\n",
    "client.activate_anomaly_detector(AnomalyDetectorArn=detector_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76be5fa9",
   "metadata": {},
   "source": [
    "client.describe_anomaly_detector(AnomalyDetectorArn=detector_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e203849",
   "metadata": {},
   "source": [
    "response = client.get_anomaly_group(\n",
    "    AnomalyGroupId='your-group-id',\n",
    "    AnomalyDetectorArn=detector_arn\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
